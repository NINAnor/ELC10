left_join(s1, by="POINT_ID")%>%
left_join(s2, by="POINT_ID")%>%
left_join(auxiliary, by="POINT_ID") %>%
mutate(LC = factor(LC, levels = c('Artificial land','Cropland', 'Woodland', 'Grassland',
'Shrubland', 'Bare land', 'Water', 'Wetland')))
#### Data checking and explore ------------------------------------------------------------
names(trainingFeats)
nonPredVars <- c('POINT_ID', 'LC', 'LC_num', 'source', 'lat', 'lon')
selectVars <- names(trainingFeats)[!names(trainingFeats) %in% nonPredVars]
selectVars
length(selectVars)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
subsets <- c(2,5,10,15,20, 30, 40)
subsets <- c(2,5)
# run the RFE algorithm
results <- rfe(trainingFeats[,selectVars], trainingFeats$LC, sizes=subsets, rfeControl=control)
colSums(is.na(trainingFeats))
# Merge into one data frame
trainingFeats <- ref_points %>%
left_join(s1, by="POINT_ID")%>%
left_join(s2, by="POINT_ID")%>%
left_join(auxiliary, by="POINT_ID") %>%
mutate(LC = factor(LC, levels = c('Artificial land','Cropland', 'Woodland', 'Grassland',
'Shrubland', 'Bare land', 'Water', 'Wetland'))) %>%
drop_na()
#### Data checking and explore ------------------------------------------------------------
names(trainingFeats)
trainingFeats %>%
ggplot(aes(x = LC, fill=source)) +
geom_bar(stat='count') +
coord_flip()
#### Data checking and explore ------------------------------------------------------------
nrow(trainingFeats)
names(trainingFeats)
trainingFeats %>%
ggplot(aes(x = LC, fill=source)) +
geom_bar(stat='count') +
coord_flip()
nonPredVars <- c('POINT_ID', 'LC', 'LC_num', 'source', 'lat', 'lon')
selectVars <- names(trainingFeats)[!names(trainingFeats) %in% nonPredVars]
selectVars
length(selectVars)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
subsets <- c(2,5)
# run the RFE algorithm
results <- rfe(trainingFeats[,selectVars], trainingFeats$LC, sizes=subsets, rfeControl=control)
control <- rfeControl(functions=rfFuncs, method="cv", number=5, verbose=TRUE)
subsets <- c(2,5)
# run the RFE algorithm
results <- rfe(trainingFeats[,selectVars], trainingFeats$LC, sizes=subsets, rfeControl=control)
#### Run final Random Forest -------------------------------------------------------------------
selectVarsFinal <- c("ndvi_p25",
"green_median",
"temp" ,
"light",
"nbr_stdDev",
"asc_vh_median" ,
"desc_vh_median",
"swir2_median" ,
"elevation" ,
"desc_dpol_median",
"R1_median",
"precip" ,
"asc_dpol_median",
"temp_stDev",
"asc_vv_stDev")
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(selectVarsFinal, 'LC'))) %>%
mutate(LC = factor(LC))
selectVars
#### Run final Random Forest -------------------------------------------------------------------
selectVarsFinal <- c("ndvi_p25",
"green_median",
"temp" ,
"light",
"nbr_stdDev",
"asc_vh_median" ,
"desc_vh_median",
"swir2_median" ,
"elev" ,
"desc_dpol_median",
"R1_median",
"precip" ,
"asc_dpol_median",
"temp_stDev",
"asc_vv_stDev")
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(selectVarsFinal, 'LC'))) %>%
mutate(LC = factor(LC))
trainingFinal
model.rf <- randomForest(LC~.,
data=trainingFinal,
ntree=100,
importance=T,
do.trace = 20)
print(model.rf)
plot(model.rf)
varImpPlot(model.rf, sort=T)
cm <- confusionMatrix(model.rf$predicted, train$LC)
cm$overall
cm <- confusionMatrix(model.rf$predicted, trainingFinal$LC)
cm$overall
cm
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(selectVarsFinal, 'LC'))) %>%
mutate(LC = factor(LC))
model.rf <- randomForest(LC~.,
data=trainingFinal,
ntree=100,
importance=T,
do.trace = 20)
print(model.rf)
plot(model.rf)
varImpPlot(model.rf, sort=T)
cm <- confusionMatrix(model.rf$predicted, trainingFinal$LC)
cm
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(selectVars, 'LC'))) %>%
mutate(LC = factor(LC))
model.rf <- randomForest(LC~.,
data=trainingFinal,
ntree=100,
importance=T,
do.trace = 20)
print(model.rf)
plot(model.rf)
varImpPlot(model.rf, sort=T)
cm
cm <- confusionMatrix(model.rf$predicted, trainingFinal$LC)
cm
training <-  trainingFeats %>%
dplyr::select(all_of(c(selectVars, 'LC'))) %>%
mutate(LC = factor(LC))
import <- data.frame()
iteration <- seq(1,10, 1)
for (i in iteration){
print(paste0('Iteration number: ', i))
model.rf <-  randomForest(LC~.,
data=training,
ntree=100,
importance=T,
do.trace = 20)
newImp <- as.data.frame(importance(model.rf))
newImp$var <- rownames(newImp)
newImp$iteration <- i
import <- bind_rows(import, newImp)
}
import <- import %>% as_tibble()
topVars <- import %>%
mutate(meanAcc = (BBmisc::normalize(MeanDecreaseAccuracy,  method = "range") +
BBmisc::normalize(MeanDecreaseGini,  method = "range"))/2) %>%
group_by(var) %>%
summarise(MeanDecreaseGini = mean(MeanDecreaseGini),
MeanDecreaseAccuracy = mean(MeanDecreaseAccuracy),
imp = mean(meanAcc),
sd = sd(meanAcc)/sqrt(n())) %>%
arrange(desc(imp))
topVars$var[1:15]
topVars
topVars %>%
ggplot(aes(x=var, y=imp)) +
geom_bar(stat='identity') + coord_flip()
topVars %>%
ggplot(aes(x=reorder(var, imp), y=imp)) +
geom_bar(stat='identity') + coord_flip()
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(topVars$var[1:15], 'LC'))) %>%
mutate(LC = factor(LC))
model.rf <- randomForest(LC~.,
data=trainingFinal,
ntree=100,
importance=T,
do.trace = 20)
print(model.rf)
plot(model.rf)
varImpPlot(model.rf, sort=T)
cm <- confusionMatrix(model.rf$predicted, trainingFinal$LC)
cm
selectVarsFinal <- topVars$var[1:15]
# Export final training dataset
trainingFeatsExport <-trainingFeats %>%
dplyr::select(all_of(c(selectVarsFinal, 'LC', 'LC_num'))) %>%
mutate(LC = factor(LC))
trainingFeatsExport
trainingFeats
# Export final training dataset
trainingFeatsExport <-trainingFeats %>%
dplyr::select(all_of(c( 'LC', 'LC_num', selectVarsFinal))) %>%
mutate(LC = factor(LC))
trainingFeatsExport
trainingFeatsExport %>%
group_by(source) %>%
summarise(n= n())
trainingFeatsExport %>%
write_csv('./DATA/For_GEE/ELC10_training_feats.csv')
trainingFinal
selectVarsFinal
topVars
View(topVars)
cm
#### Run final Random Forest -------------------------------------------------------------------
selectVarsFinal <- c("ndvi_p25",
"green_median",
"temp" ,
"light",
"nbr_stdDev",
"asc_vh_median" ,
"desc_vh_median",
"swir2_median" ,
"elev" ,
"desc_dpol_median",
"R1_median",
"precip" ,
"asc_dpol_median",
"temp_stDev",
"asc_vv_stDev")
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(selectVarsFinal, 'LC'))) %>%
mutate(LC = factor(LC))
model.rf <- randomForest(LC~.,
data=trainingFinal,
ntree=100,
importance=T,
do.trace = 20)
cm <- confusionMatrix(model.rf$predicted, trainingFinal$LC)
cm
topVars %>%
ggplot(aes(x=reorder(var, imp), y=imp)) +
geom_bar(stat='identity') + coord_flip()
topVars %>%
filter(var %in% topVars$var[1:15]) %>%
ggplot(aes(x=reorder(var, imp), y=imp)) +
geom_bar(stat='identity') + coord_flip()
topVars %>%
filter(var %in% topVars$var[1:15]) %>%
ggplot(aes(x=reorder(var, imp), y=imp*100)) +
geom_bar(stat='identity') + coord_flip()
topVars %>%
filter(var %in% topVars$var[1:15]) %>%
ggplot(aes(x=reorder(var, imp), y=imp*100)) +
geom_bar(stat='identity') + coord_flip() +
xlab("Importance (%)") +
ylab('')
topVars %>%
filter(var %in% topVars$var[1:15]) %>%
ggplot(aes(x=reorder(var, imp), y=imp*100)) +
geom_bar(stat='identity') + coord_flip() +
ylab("Importance (%)") +
xlab('')
#### Data checking and explore ------------------------------------------------------------
# See distribution of reference data
#
ref_points %>%
ggplot(aes(x = LC, fill=source)) +
geom_bar(stat='count') +
coord_flip()
# Number of training features
nrow(trainingFeats)
# Number of training features by data source
trainingFeats %>%
group_by(source) %>%
summarise(n=n())
nrow(trainingFeats)
# Number of training features by data source
ref_points %>%
group_by(source) %>%
summarise(n=n())
nrow(ref_points)
# Number of training features by data source
ref_points %>%
group_by(source) %>%
summarise(n=n())
#### Setup environment ------------------------------------------------------------------------
# Import libraryies (install if not already installed)
library(tidyverse)
library(randomForest)
library(caret)
library(sf)
# Set plotting theme for ggplot
theme_set(theme_bw()+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
theme(strip.background =element_rect(fill="white")))
# Function to read in multiple files from directory
readMultiFiles <- function(directory){
files <- list.files(directory, pattern='*.csv', full.names=TRUE)
raw <- files %>%
map_df(~read_csv(.))
return (raw)
}
# Auxiliary data from GEE
auxiliary <- read_csv('./DATA/From_GEE/Auxiliary/auxiliary_data.csv') %>%
dplyr::select(-'system:index', -'.geo', -LC, -LC_num, -votes, -source)
auxiliary
LUCAS polygon & point land cover reference points
ref_points <- read_csv('./DATA/For_GEE/LUCAS_combined_reference_final.csv') %>%
dplyr::select(-votes)
# Sentinel 1 SAR data from GEE
s1 <- readMultiFiles('./DATA/From_GEE/Sentinel/S1_data/') %>%
dplyr::select(-'system:index', -'.geo', -LC, -LC_num, -votes, -source, -X1)
# Sentinel 2 optical data from GEE
s2 <- readMultiFiles('./DATA/From_GEE/Sentinel/S2_data/') %>%
dplyr::select(-'system:index', -'.geo', -LC, -LC_num, -votes, -source, -X1)
# Auxiliary data from GEE
auxiliary <- read_csv('./DATA/From_GEE/Auxiliary/auxiliary_data.csv') %>%
dplyr::select(-'system:index', -'.geo', -LC, -LC_num, -votes, -source)
# Merge into one data frame
trainingFeats <- ref_points %>%
left_join(s1, by="POINT_ID")%>%
left_join(s2, by="POINT_ID")%>%
left_join(auxiliary, by="POINT_ID") %>%
mutate(LC = factor(LC, levels = c('Artificial land','Cropland', 'Woodland', 'Grassland',
'Shrubland', 'Bare land', 'Water', 'Wetland'))) %>%
drop_na()
# Hard coding in the top variables for illustrative purposes
selectVarsFinal <- c("ndvi_p25",
"green_median",
"temp" ,
"light",
"nbr_stdDev",
"asc_vh_median" ,
"desc_vh_median",
"swir2_median" ,
"elevation" ,
"desc_dpol_median",
"R1_median",
"precip" ,
"asc_dpol_median",
"temp_stDev",
"asc_vv_stDev")
# Create final training dataset
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(selectVarsFinal, 'LC'))) %>%
mutate(LC = factor(LC))
# Run RF model (ntree and default mtry have been tuned based on previous scripts)
model.rf <- randomForest(LC~.,
data=trainingFinal,
ntree=100,
importance=T,
do.trace = 20)
# Confusion matrix
cm <- confusionMatrix(model.rf$predicted, trainingFinal$LC)
cm
# Export final training dataset for upload to GEE
trainingFeatsExport <-trainingFeats %>%
dplyr::select(all_of(c( 'LC', 'LC_num', selectVarsFinal))) %>%
mutate(LC = factor(LC))
trainingFeatsExport %>%
write_csv('./DATA/For_GEE/ELC10_training_feats.csv')
trainingFeatsExport
# Create training dataset for RF model
training <-  trainingFeats %>%
dplyr::select(all_of(c(selectVars, 'LC'))) %>%
mutate(LC = factor(LC))
# Empty data frame for importance scores
import <- data.frame()
iteration <- seq(1,10, 1)
# Run for loop to generate importance scores
for (i in iteration){
print(paste0('Iteration number: ', i))
model.rf <-  randomForest(LC~.,
data=training,
ntree=100,
importance=T,
do.trace = 20)
newImp <- as.data.frame(importance(model.rf))
newImp$var <- rownames(newImp)
newImp$iteration <- i
import <- bind_rows(import, newImp)
}
import <- import %>% as_tibble()
# Calculate varibale importance as the average between normalized Gini and accuracy
topVars <- import %>%
mutate(meanAcc = (BBmisc::normalize(MeanDecreaseAccuracy,  method = "range") +
BBmisc::normalize(MeanDecreaseGini,  method = "range"))/2) %>%
group_by(var) %>%
summarise(MeanDecreaseGini = mean(MeanDecreaseGini),
MeanDecreaseAccuracy = mean(MeanDecreaseAccuracy),
imp = mean(meanAcc),
sd = sd(meanAcc)/sqrt(n())) %>%
arrange(desc(imp))
trainingFeats
# List non-predictor variables
nonPredVars <- c('POINT_ID', 'LC', 'LC_num', 'source', 'lat', 'lon')
# List predictor variables
selectVars <- names(trainingFeats)[!names(trainingFeats) %in% nonPredVars]
selectVars
length(selectVars)
# Define Recursive Feature Selection (RFE) control parameters
control <- rfeControl(functions=rfFuncs, method="cv", number=5, verbose=TRUE)
# Create training dataset for RF model
training <-  trainingFeats %>%
dplyr::select(all_of(c(selectVars, 'LC'))) %>%
mutate(LC = factor(LC))
# Empty data frame for importance scores
import <- data.frame()
iteration <- seq(1,10, 1)
# Run for loop to generate importance scores
for (i in iteration){
print(paste0('Iteration number: ', i))
model.rf <-  randomForest(LC~.,
data=training,
ntree=100,
importance=T,
do.trace = 20)
newImp <- as.data.frame(importance(model.rf))
newImp$var <- rownames(newImp)
newImp$iteration <- i
import <- bind_rows(import, newImp)
}
import <- import %>% as_tibble()
# Calculate varibale importance as the average between normalized Gini and accuracy
topVars <- import %>%
mutate(meanAcc = (BBmisc::normalize(MeanDecreaseAccuracy,  method = "range") +
BBmisc::normalize(MeanDecreaseGini,  method = "range"))/2) %>%
group_by(var) %>%
summarise(MeanDecreaseGini = mean(MeanDecreaseGini),
MeanDecreaseAccuracy = mean(MeanDecreaseAccuracy),
imp = mean(meanAcc),
sd = sd(meanAcc)/sqrt(n())) %>%
arrange(desc(imp))
# Top 15 variables
topVars$var[1:15]
# PLot them
topVars %>%
filter(var %in% topVars$var[1:15]) %>%
ggplot(aes(x=reorder(var, imp), y=imp*100)) +
geom_bar(stat='identity') + coord_flip() +
ylab("Importance (%)") +
xlab('')
# Top 15 variables
topVars$var[1:15]
selectVarsFinal <-c("green_median",
"nbr_stdDev",
"light",
"asc_vh_median",
"temp",
"ndvi_p95",
"ndvi_p25",
"elevation",
"desc_vh_median",
"swir2_median" ,
"precip" ,
"temp_stDev" ,
"desc_vv_median",
"red_median" ,
"R1_median" )
# Create final training dataset
trainingFinal <- trainingFeats %>%
dplyr::select(all_of(c(selectVarsFinal, 'LC'))) %>%
mutate(LC = factor(LC))
# Run RF model (ntree and default mtry have been tuned based on previous scripts)
model.rf <- randomForest(LC~.,
data=trainingFinal,
ntree=100,
importance=T,
do.trace = 20)
# Confusion matrix
cm <- confusionMatrix(model.rf$predicted, trainingFinal$LC)
cm
# Export final training dataset for upload to GEE
trainingFeatsExport <-trainingFeats %>%
#dplyr::select(all_of(c( 'LC', 'LC_num', selectVarsFinal))) %>%
mutate(LC = factor(LC))
trainingFeatsExport %>%
write_csv('./DATA/For_GEE/ELC10_training_feats.csv')
# Hard coding in the top variables for illustrative purposes
selectVarsFinal <- c("ndvi_p25",
"green_median",
"temp" ,
"light",
"nbr_stdDev",
"asc_vh_median" ,
"desc_vh_median",
"swir2_median" ,
"elevation" ,
"desc_dpol_median",
"R1_median",
"precip" ,
"asc_dpol_median",
"temp_stDev",
"asc_vv_stDev")
# Export final training dataset for upload to GEE
trainingFeatsExport <-trainingFeats %>%
dplyr::select(all_of(c( 'LC', 'LC_num', selectVarsFinal))) %>%
mutate(LC = factor(LC))
trainingFeatsExport %>%
write_csv('./DATA/For_GEE/ELC10_training_feats.csv')
#### Setup environment ------------------------------------------------------------------------
# Import libraries (install if not already installed)
library(tidyverse)
library(randomForest)
library(caret)
library(sf)
# Set plotting theme for ggplot
theme_set(theme_bw()+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
theme(strip.background =element_rect(fill="white")))
# Function to read in multiple files from directory
readMultiFiles <- function(directory){
files <- list.files(directory, pattern='*.csv', full.names=TRUE)
raw <- files %>%
map_df(~read_csv(.))
return (raw)
}
#### Import data ------------------------------------------------------------------------------
# LUCAS polygon & point land cover reference points
ref_points <- read_csv('./DATA/For_GEE/LUCAS_combined_reference_final.csv') %>%
dplyr::select(-votes)
# Number of training features by data source
ref_points %>%
group_by(source) %>%
summarise(n=n())
